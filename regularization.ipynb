{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc153411-0484-43eb-a77e-d2695c5ce690",
   "metadata": {},
   "source": [
    "#  Regularizing Neural Networks \n",
    "\n",
    "* Neural networks learn a set of weights that best map inputs to outputs.\n",
    "* Deep learning neural networks are likely to quickly overfit a training dataset with few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65db5a-eaa0-4206-946e-2bc0a62ce420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d1d3ace-5e14-4419-8708-2ec8a777cb89",
   "metadata": {},
   "source": [
    "## Weight penalties\n",
    "\n",
    "* A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output. This can be a sign that the network has overfit the training dataset and will likely perform poorly when making predictions on new data.\n",
    "* A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called weight regularization and it can be used as a general technique to reduce overfitting of the training dataset and improve the generalization of the model.\n",
    "* We will mainly explore L1 regularization and L2 regularization.\n",
    "\n",
    "### L1 norm and L2 norm\n",
    "\n",
    "* Before delving into L1/L2 regularization, let's spend a bit of time introducing L1/L2 norm.\n",
    "* In mathematics, a \"norm\" is a function from vector space to the non-negative real numbers that behaves in certain ways \"like the distance from the origin\".\n",
    "* L1 norm:\n",
    "\n",
    "$$\n",
    "\\left\\lVert x \\right\\rVert_1 = \\sum_{i=1}^n \\left| x_i \\right|\n",
    "$$\n",
    "\n",
    "* L2 norm:\n",
    "\n",
    "$$\n",
    "\\left\\lVert x \\right\\rVert_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n",
    "$$\n",
    "\n",
    "* Function graph (one of the interesting differences is how their derivatives change when $w$ changes):\n",
    "\n",
    "<img src=\"./assets/regularization/l1-and-l2-norms.gif\" alt=\"L1 and L2 norms\" style=\"width: 500px;\"/>\n",
    "\n",
    "### L1 regularization\n",
    "\n",
    "* The formula for MSE with L1 regularization is:\n",
    "\n",
    "$$\n",
    "MSE_{L1} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}i)^2 + \\lambda \\sum_{j=1}^{p}(|w_j|)\n",
    "$$\n",
    "\n",
    "* where:\n",
    "\n",
    "    * $n$ is the number of samples in the dataset\n",
    "    * $y_i$ is the true target value for the $i$-th sample\n",
    "    * $\\hat{y}_i$ is the predicted target value for the $i$-th sample\n",
    "    * $p$ is the number of weights in the dataset\n",
    "    * $w_j$ is the coefficient of the $j$-th weight in the model\n",
    "    * $\\lambda$ is the regularization parameter, which controls the strength of the L1 regularization penalty\n",
    "\n",
    "* The general idea is that apart from minimizing the loss function, we also want to penalize the L1 \"distance\" from the weight vector to origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941ffdd-2607-4888-8cfd-afe1937ed619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e7d010a-2471-4f72-8ba1-604a71b9da26",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "* Ensembles of neural networks (i.e., a large number of neural networks) with different model configurations are known to reduce overfitting, but require the additional computational expense of training and maintaining multiple models.\n",
    "* A single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during **training**.\n",
    "* Dropout is implemented per-layer in a neural network. It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long short-term memory network layer.\n",
    "* Dropout can be used after convolutional layers (e.g. Conv2D) and after pooling layers (e.g. MaxPooling2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912ed8a-2d74-448e-a6c6-51bfb6784203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a53c8d32-4abb-4dd7-84f1-2251327f0033",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Use Weight Regularization to Reduce Overfitting of Deep Learning Models](https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/)\n",
    "* [A Gentle Introduction to Dropout for Regularizing Deep Neural Networks](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)\n",
    "* [How to Reduce Overfitting With Dropout Regularization in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/)\n",
    "* [How to Reduce Overfitting Using Weight Constraints in Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-neural-networks-with-weight-constraints-in-keras/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
